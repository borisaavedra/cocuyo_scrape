{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bitcbf4ef2d3404447197555e7cebfa4f00",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_menu_links(soup):\n",
    "    '''Extract URL from EFECTOCOCUYO.COM site main menu'''\n",
    "    items = soup.find(\"div\", attrs={\"class\":\"list mtopmob\"})\n",
    "    a_tags = items.find_all(\"a\")\n",
    "    link_list = [link.get(\"href\") for link in a_tags]\n",
    "\n",
    "    return link_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_links(soup):\n",
    "    '''Extract URL from carrousel news from page on EFECTOCOCUYO.COM site'''\n",
    "    feature_news = soup.find(\"div\", attrs={\"class\":\"carousel-inner\"})\n",
    "    a_list = feature_news.select(\"#carouselExampleControls > div > div.carousel-item > div.contentImage.bloque-destacado > div > a\")\n",
    "    link_list = [link.get(\"href\") for link in a_list]\n",
    "    return link_list\n",
    "\n",
    "def get_big_news(soup):\n",
    "    '''Extract URL from big news from page on EFECTOCOCUYO.COM site'''\n",
    "    feature_new = soup.find(\"div\", attrs={\"class\":\"contenttext text-center\"})\n",
    "    link = feature_new.a.get(\"href\")\n",
    "\n",
    "    return link\n",
    "\n",
    "def get_small_news(soup):\n",
    "    '''Extract all URL from carrousel news from page on EFECTOCOCUYO.COM site'''\n",
    "    all_links = []\n",
    "    try:\n",
    "        all_rows = soup.find(\"div\", attrs={\"class\":\"col col-sm-9\"})\n",
    "        a_tag = all_rows.select(\"body > section.home.d-none.d-sm-block.d-md-block.d-lg-block.d-xl-block > div.container > div.row > div.col.col-sm-9 > div > div > div > div.contentbox.text-center > a:nth-child(2)\")\n",
    "        all_links = [link.get(\"href\") for link in a_tag]\n",
    "    except Exception as e:\n",
    "        all_links.append(e)\n",
    "    return all_links\n",
    "\n",
    "print(get_feature_links(cocuyo_politics_soup), end=\"\\n\\n\")\n",
    "print(get_big_news(cocuyo_politics_soup), end=\"\\n\\n\")\n",
    "print(get_small_news(cocuyo_politics_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_news = \"https://efectococuyo.com/politica/carmen-b-fernandez-urge-punto-de-encuentro-entre-propuestas-de-capriles-y-guaido-conlaluz/\"\n",
    "\n",
    "def get_news_info(url_news):\n",
    "    '''Extract title, date and content from the EFECTOCOCUYO.COM's articles'''\n",
    "    data_news = {}\n",
    "    try:\n",
    "        news = requests.get(url_news)\n",
    "        if news.status_code != 200:\n",
    "            print(f\"Error: {url_news} not found\")\n",
    "        else:\n",
    "            soup_news = BeautifulSoup(news.text, \"html.parser\")\n",
    "            title = soup_news.find(\"h1\", attrs={\"class\":\"fontbree\"})\n",
    "            # Extract category and time from the date (h3) #\n",
    "            date = soup_news.find(\"h3\")\n",
    "            all_span = date.find_all(\"span\")\n",
    "            for element in all_span:\n",
    "                element.extract()\n",
    "            #################################################\n",
    "            body_news = soup_news.find(\"div\", attrs={\"class\":\"col-8 col-sm-7 col-lg-9\"})\n",
    "            data_news[\"title\"] = title.text\n",
    "            data_news[\"date\"] = date.text\n",
    "            data_news[\"body\"] = body_news.text\n",
    "            return data_news\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return data_news\n",
    "\n",
    "def pretty(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      else:\n",
    "         print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "pretty(get_news_info(url_news))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://efectococuyo.com/\"\n",
    "try:\n",
    "    cocuyo = requests.get(url)\n",
    "    cocuyo_soup = BeautifulSoup(cocuyo.text, \"html.parser\")\n",
    "except Exception as e:\n",
    "    print(f\"----- ERROR getting the main soup {url} ---------\")\n",
    "\n",
    "def scrape_cocuyo(cocuyo_soup):\n",
    "    '''Scrapping EFECTOCOCUYO.COM site'''\n",
    "    notes = []\n",
    "    all_data = []\n",
    "    menu_link_list = get_menu_links(cocuyo_soup)\n",
    "\n",
    "    for link in menu_link_list:\n",
    "        try:\n",
    "            link_response = requests.get(link)\n",
    "        except Exception as e:\n",
    "            print(f\"----> Error getting response {link}: {e}\")\n",
    "            break\n",
    "\n",
    "        if link_response.status_code == 200:\n",
    "            soup = BeautifulSoup(link_response.text, \"html.parser\")\n",
    "            for link in get_feature_links(soup):\n",
    "                notes.append(link)\n",
    "            notes.append(get_big_news(soup))\n",
    "            for link in get_small_news(soup):\n",
    "                notes.append(link)\n",
    "        \n",
    "    for i, url in enumerate(notes, 1):\n",
    "        print(f\"Scrapping note{i} from {len(notes)}\")\n",
    "        all_data.append(get_news_info(url))    \n",
    "    \n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = scrape_cocuyo(cocuyo_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"efecto_cocuyo_notes_20200915.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}